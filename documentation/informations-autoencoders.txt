Tipos de autoencoder:
- Convolutional Autoencoder
- Sparse Autoencoder
- Deep Autoencoders
- Contractive Autoencoders

Types of Autoencoders:
Vanilla autoencoders
Multilayer autoencoder
Convolutional autoencoder
Regularized autoencoder

Types of Autoencoders:
1. Denoising autoencoder
2. Sparse autoencoder
3. Variational autoencoder (VAE)
4. Convolutional autoencoder (CAE)

https://github.com/nathanhubens/Autoencoders/blob/master/Autoencoders.ipynb
https://codeburst.io/deep-learning-types-and-autoencoders-a40ee6754663


PCA só pode ser transformado linearmente
Autoencoders although is quite similar to PCA but its Autoencoders are much more flexible than PCA. Autoencoders can represent both liners and non-linar transformation in encoding but PCA can only perform linear transformation. Autoencoders can be layered to form deep learning network due to it’s Network representation.

Propriedades de autoencoders:
- Unsupervised: Autoencoders são considerados uma técnica sem supervisão desde que não seja preciso deixar explícito as legendas utilizadas para treino.
- Data-specific: Autoencoders consideradas apenas capazes de compressar dados similares ao dados utilizados no treinamento.
- Lossy: A saída não será exatamente a mesma que a entrada, o que causa perda e degradação na representação

Training Autoencoders:
- Code size: Pequenos tamanhos resultam em maior compressão
- Number of layers: A compressão precisa ter muitas layers
- Loss function: Mean squared error or binary cross entropy
- Number of node per layers: Stacked autoencoders look like a sandwich

Encoder:
- Encoder é uma rede neural cuja a saída representa z de dados x.

Decoder:
- Decoder é uma rede neural que aprende a reconstruir os dados de x a partir da representação de z.

--------------------------------------------
- Convolutional Autoencoder: 
Use the convolution operator to learn to encode the input in a set of simple signals and then try to reconstruct the input from them.

Image Reconstruction:
- Remove noise and reconstructs missing parts.
- Noisy version is converted to clean version
- the network fills the gaps in the image

Image colorization:
- maps circles and squares from an image to the same image but with colors
- purple is formed sometimes because of blend of colors, where network hesitates between circle or square

Advanced applications:
- fully image colorization
- latent space clustering
- generating higher resolution images

---------------------------------------------
Sparse Autoencoder:
Offer us an alternative method for introducing an information bottleneck without requiring a reduction in the number of nodes at our hidden layers.

Regulariza-se o tamanho da rede neural e não dá ativação.

---------------------------------------------
Deep Autoencoders:
Is composed of two, symmetrical deep-belief networks:
- First four or five shallow layers representing the encoding half of the network neural
- Second set of four or five layers that make up the decoding half.

The first layer of the Deep Autoencoder learns first-order features  in the raw input such as edges in an image.

The second layer learns second-order features corresponding to patterns in the appearence of first-order features.

Supervisionada.

______________________________________________
Contractive Autoencoders:
Is an unsupervised deep learning technique that helps a neural network encode unlabled training data.

Similar inputs are contracted to a constant output within a neighborhood, based on what the model observed during training.


- Image coloring
- Dimensionality reduction
- Denoising images
- Watermark Removal

Variational Autoencoder:
- Entangled: Forçar o mapeamento das informações.
- Lidar com diferentes entradas/vetores. 

- Encoder
- Bottleneck: Vector to Sample from a Distribution
- Decoder

- Activation: Decoder

PCA: A Análise de Componentes Principais (ACP) ou Principal Component Analysis (PCA) é um procedimento matemático que utiliza uma transformação ortogonal (ortogonalização de vetores) para converter um conjunto de observações de variáveis possivelmente correlacionadas num conjunto de valores de variáveis linearmente não correlacionadas chamadas de componentes principais. O número de componentes principais é sempre menor ou igual ao número de variáveis originais. Os componentes principais são garantidamente independentes apenas se os dados forem normalmente distribuídos (conjuntamente). O PCA é sensível à escala relativa das variáveis originais. Dependendo da área de aplicação, o PCA é também conhecido como transformada de Karhunen-Loève (KLT) discreta, transformada de Hotelling ou decomposição ortogonal própria (POD).

- Reguralized autoencoders
- Vanilla autoencoders

Autoencoder: O número de neurônios é o mesmo na entrada e saída, o tamanho da imagem é reduzido, mas a imagem permanece igual. Criação de representações que podem ser compressadas.

What is autoencoder:
https://medium.com/datadriveninvestor/deep-learning-autoencoders-db265359943e

Deep Learning — Different Types of Autoencoders:
https://medium.com/datadriveninvestor/deep-learning-different-types-of-autoencoders-41d4fa5f7570

- Undercomplete Autoencoders
- Sparse Autoencoders
- Denoising Autoencoders(DAE)
- Contractive Autoencoders(CAE)
- Stacked Denoising Autoencoders
- Deep Autoencoders

Example using Keras:
https://medium.com/datadriveninvestor/deep-autoencoder-using-keras-b77cd3e8be95




